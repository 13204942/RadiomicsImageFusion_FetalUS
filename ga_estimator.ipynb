{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D-CNN Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from src.one_d_cnn import *\n",
    "from src.one_d_transformer import *\n",
    "from src.mlp import *\n",
    "from src.one_d_attention import *\n",
    "\n",
    "import sklearn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fafe4292750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1234)\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_FLAG = 'cnn'\n",
    "# MODEL_FLAG = 'transformer'\n",
    "# MODEL_FLAG = 'mlp'\n",
    "MODEL_FLAG = 'attention'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'attention':\n",
    "    model = OneAttention()\n",
    "    model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, seq, feature\n",
    "src = torch.rand(16, 1, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "if MODEL_FLAG == 'attention':\n",
    "    model = model.to(device)\n",
    "    src = src.to(device)\n",
    "    out = model(src)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'transformer':\n",
    "    model = OneDTransformer(\n",
    "        n_classes=256, \n",
    "        n_label=1,\n",
    "        n_length=95,\n",
    "        n_channel=1,\n",
    "        vocab_size=100, \n",
    "        num_embed=64,\n",
    "        embed_size=256,\n",
    "        nhead=4, \n",
    "        dim_feedforward=512, \n",
    "        dropout=0.25,\n",
    "        activation='relu',\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, seq, feature\n",
    "src = torch.rand(16, 1, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'transformer':\n",
    "    model = model.to(device)\n",
    "    src = src.to(device)\n",
    "    out = model(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1D-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, seq, feature\n",
    "src = torch.rand(16, 1, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'cnn':\n",
    "    model = OneDCNN()\n",
    "    src = src.to(device)\n",
    "    out = model(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'mlp':\n",
    "    model = OurMLP(num_classes=1, in_channels=95)\n",
    "    model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "if MODEL_FLAG == 'cnn':\n",
    "    test_x = torch.rand(1, 1, 95)\n",
    "    test_y = model(test_x)\n",
    "    print(test_y.shape)\n",
    "    test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_FLAG == 'mlp':\n",
    "    test_x = torch.rand(1, 1, 95)\n",
    "    test_y = model(test_x)\n",
    "    print(test_y.shape)\n",
    "    test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.custom_dataset import RadiomicDataset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/mnt/storage/fangyijie/radiomics_ga'\n",
    "\n",
    "df_dir = root_dir + '/ga_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(os.path.join(df_dir, 'X_train.pkl'))\n",
    "y_train = pd.read_pickle(os.path.join(df_dir, 'y_train.pkl'))\n",
    "\n",
    "X_test = pd.read_pickle(os.path.join(df_dir, 'X_test.pkl'))\n",
    "y_test = pd.read_pickle(os.path.join(df_dir, 'y_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore column 'image_name'\n",
    "# convert dataframe to matrix\n",
    "src_X_array = X_train.iloc[:, :-1].to_numpy()\n",
    "src_y_array = y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardizing data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(src_X_array)\n",
    "src_X_array = scaler.transform(src_X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = RadiomicDataset(src_X_array, src_y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  torch.Size([16, 1, 95]) torch.float32\n",
      "Shape of y:  torch.Size([16, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "for X, y in train_dataloader:\n",
    "        print(\"Shape of X: \", X.shape, X.dtype)\n",
    "        # Output: Shape of X:  torch.Size([1, 1, 543]) torch.float64\n",
    "        print(\"Shape of y: \", y.shape, y.dtype)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "lr = 0.0001\n",
    "# lr = 0.0001 # MLP\n",
    "k_folds = 5\n",
    "criterion = nn.MSELoss() # MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/storage/fangyijie/radiomics_ga/logs/ga/attention'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GA\n",
    "if MODEL_FLAG == 'cnn':\n",
    "    log_dir = root_dir + '/logs/ga/1d_cnn'\n",
    "\n",
    "if MODEL_FLAG == 'transformer':\n",
    "    log_dir = root_dir + '/logs/ga/1d_transformer'\n",
    "\n",
    "if MODEL_FLAG == 'mlp':\n",
    "    log_dir = root_dir + '/logs/ga/mlp'\n",
    "\n",
    "if MODEL_FLAG == 'attention':\n",
    "    log_dir = root_dir + '/logs/ga/attention'\n",
    "\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "training loss: 180.400011\n",
      "Validation loss: 161.836274 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "training loss: 156.439828\n",
      "Validation loss: 143.811010 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "training loss: 117.522630\n",
      "Validation loss: 87.107979 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "training loss: 69.599909\n",
      "Validation loss: 55.990435 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "training loss: 54.107243\n",
      "Validation loss: 52.405451 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "training loss: 50.674594\n",
      "Validation loss: 48.794786 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "training loss: 47.894067\n",
      "Validation loss: 45.210033 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "training loss: 44.798193\n",
      "Validation loss: 44.398726 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "training loss: 42.448318\n",
      "Validation loss: 43.698689 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "training loss: 42.524070\n",
      "Validation loss: 42.261160 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "training loss: 40.661266\n",
      "Validation loss: 40.434892 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "training loss: 40.922393\n",
      "Validation loss: 40.623249 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "training loss: 39.550264\n",
      "Validation loss: 41.923964 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "training loss: 39.657576\n",
      "Validation loss: 39.423217 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "training loss: 39.430037\n",
      "Validation loss: 39.138798 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "training loss: 38.546612\n",
      "Validation loss: 39.551744 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "training loss: 37.052308\n",
      "Validation loss: 37.865918 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "training loss: 37.331365\n",
      "Validation loss: 37.886824 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "training loss: 37.645870\n",
      "Validation loss: 37.725269 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "training loss: 36.202832\n",
      "Validation loss: 35.864450 \n",
      "\n",
      "Epoch 21\n",
      "----------------------------\n",
      "training loss: 37.903188\n",
      "Validation loss: 37.162918 \n",
      "\n",
      "Epoch 22\n",
      "----------------------------\n",
      "training loss: 35.216595\n",
      "Validation loss: 36.089514 \n",
      "\n",
      "Epoch 23\n",
      "----------------------------\n",
      "training loss: 36.397604\n",
      "Validation loss: 36.207262 \n",
      "\n",
      "Epoch 24\n",
      "----------------------------\n",
      "training loss: 36.317022\n",
      "Validation loss: 35.945790 \n",
      "\n",
      "Epoch 25\n",
      "----------------------------\n",
      "training loss: 35.408702\n",
      "Validation loss: 37.484150 \n",
      "\n",
      "Epoch 26\n",
      "----------------------------\n",
      "training loss: 35.584025\n",
      "Validation loss: 35.018917 \n",
      "\n",
      "Epoch 27\n",
      "----------------------------\n",
      "training loss: 35.854815\n",
      "Validation loss: 36.064679 \n",
      "\n",
      "Epoch 28\n",
      "----------------------------\n",
      "training loss: 35.730482\n",
      "Validation loss: 36.140142 \n",
      "\n",
      "Epoch 29\n",
      "----------------------------\n",
      "training loss: 34.874080\n",
      "Validation loss: 35.494819 \n",
      "\n",
      "Epoch 30\n",
      "----------------------------\n",
      "training loss: 34.263218\n",
      "Validation loss: 36.657938 \n",
      "\n",
      "Epoch 31\n",
      "----------------------------\n",
      "training loss: 35.908951\n",
      "Validation loss: 35.976861 \n",
      "\n",
      "Epoch 32\n",
      "----------------------------\n",
      "training loss: 34.643294\n",
      "Validation loss: 34.967823 \n",
      "\n",
      "Epoch 33\n",
      "----------------------------\n",
      "training loss: 34.705079\n",
      "Validation loss: 34.624982 \n",
      "\n",
      "Epoch 34\n",
      "----------------------------\n",
      "training loss: 35.067544\n",
      "Validation loss: 34.796033 \n",
      "\n",
      "Epoch 35\n",
      "----------------------------\n",
      "training loss: 33.206427\n",
      "Validation loss: 36.484163 \n",
      "\n",
      "Epoch 36\n",
      "----------------------------\n",
      "training loss: 34.188358\n",
      "Validation loss: 35.820524 \n",
      "\n",
      "Epoch 37\n",
      "----------------------------\n",
      "training loss: 33.120313\n",
      "Validation loss: 35.186637 \n",
      "\n",
      "Epoch 38\n",
      "----------------------------\n",
      "training loss: 33.631556\n",
      "Validation loss: 36.856365 \n",
      "\n",
      "Epoch 39\n",
      "----------------------------\n",
      "training loss: 33.881339\n",
      "Validation loss: 34.414203 \n",
      "\n",
      "Epoch 40\n",
      "----------------------------\n",
      "training loss: 34.830849\n",
      "Validation loss: 34.355939 \n",
      "\n",
      "Epoch 41\n",
      "----------------------------\n",
      "training loss: 33.492639\n",
      "Validation loss: 34.431070 \n",
      "\n",
      "Epoch 42\n",
      "----------------------------\n",
      "training loss: 33.145251\n",
      "Validation loss: 35.467087 \n",
      "\n",
      "Epoch 43\n",
      "----------------------------\n",
      "training loss: 33.265763\n",
      "Validation loss: 34.900397 \n",
      "\n",
      "Epoch 44\n",
      "----------------------------\n",
      "training loss: 33.017140\n",
      "Validation loss: 35.929976 \n",
      "\n",
      "Epoch 45\n",
      "----------------------------\n",
      "training loss: 32.950435\n",
      "Validation loss: 35.167234 \n",
      "\n",
      "Epoch 46\n",
      "----------------------------\n",
      "training loss: 32.436621\n",
      "Validation loss: 34.170651 \n",
      "\n",
      "Epoch 47\n",
      "----------------------------\n",
      "training loss: 32.404432\n",
      "Validation loss: 33.944250 \n",
      "\n",
      "Epoch 48\n",
      "----------------------------\n",
      "training loss: 31.711822\n",
      "Validation loss: 36.485060 \n",
      "\n",
      "Epoch 49\n",
      "----------------------------\n",
      "training loss: 31.636236\n",
      "Validation loss: 36.117542 \n",
      "\n",
      "Epoch 50\n",
      "----------------------------\n",
      "training loss: 31.398454\n",
      "Validation loss: 35.585374 \n",
      "\n",
      "Epoch 51\n",
      "----------------------------\n",
      "training loss: 32.877324\n",
      "Validation loss: 34.502180 \n",
      "\n",
      "Epoch 52\n",
      "----------------------------\n",
      "training loss: 32.086177\n",
      "Validation loss: 34.689787 \n",
      "\n",
      "Epoch 53\n",
      "----------------------------\n",
      "training loss: 31.072584\n",
      "Validation loss: 35.668407 \n",
      "\n",
      "Epoch 54\n",
      "----------------------------\n",
      "training loss: 31.791347\n",
      "Validation loss: 32.345166 \n",
      "\n",
      "Epoch 55\n",
      "----------------------------\n",
      "training loss: 31.538039\n",
      "Validation loss: 34.271935 \n",
      "\n",
      "Epoch 56\n",
      "----------------------------\n",
      "training loss: 31.620777\n",
      "Validation loss: 33.663456 \n",
      "\n",
      "Epoch 57\n",
      "----------------------------\n",
      "training loss: 31.278472\n",
      "Validation loss: 34.037442 \n",
      "\n",
      "Epoch 58\n",
      "----------------------------\n",
      "training loss: 31.785196\n",
      "Validation loss: 35.029699 \n",
      "\n",
      "Epoch 59\n",
      "----------------------------\n",
      "training loss: 31.005175\n",
      "Validation loss: 33.208034 \n",
      "\n",
      "Epoch 60\n",
      "----------------------------\n",
      "training loss: 30.365792\n",
      "Validation loss: 35.564884 \n",
      "\n",
      "Epoch 61\n",
      "----------------------------\n",
      "training loss: 31.513713\n",
      "Validation loss: 34.656346 \n",
      "\n",
      "Epoch 62\n",
      "----------------------------\n",
      "training loss: 31.342641\n",
      "Validation loss: 34.897880 \n",
      "\n",
      "Epoch 63\n",
      "----------------------------\n",
      "training loss: 30.815648\n",
      "Validation loss: 33.396656 \n",
      "\n",
      "Epoch 64\n",
      "----------------------------\n",
      "training loss: 30.603852\n",
      "Validation loss: 34.948053 \n",
      "\n",
      "Epoch 65\n",
      "----------------------------\n",
      "training loss: 30.215529\n",
      "Validation loss: 35.618524 \n",
      "\n",
      "Epoch 66\n",
      "----------------------------\n",
      "training loss: 31.367716\n",
      "Validation loss: 36.320426 \n",
      "\n",
      "Epoch 67\n",
      "----------------------------\n",
      "training loss: 30.699721\n",
      "Validation loss: 34.589379 \n",
      "\n",
      "Epoch 68\n",
      "----------------------------\n",
      "training loss: 30.437713\n",
      "Validation loss: 34.340123 \n",
      "\n",
      "Epoch 69\n",
      "----------------------------\n",
      "training loss: 30.201461\n",
      "Validation loss: 35.710361 \n",
      "\n",
      "Epoch 70\n",
      "----------------------------\n",
      "training loss: 29.817449\n",
      "Validation loss: 34.309560 \n",
      "\n",
      "Epoch 71\n",
      "----------------------------\n",
      "training loss: 30.364079\n",
      "Validation loss: 33.980076 \n",
      "\n",
      "Epoch 72\n",
      "----------------------------\n",
      "training loss: 30.183464\n",
      "Validation loss: 33.489353 \n",
      "\n",
      "Epoch 73\n",
      "----------------------------\n",
      "training loss: 29.703119\n",
      "Validation loss: 33.717578 \n",
      "\n",
      "Epoch 74\n",
      "----------------------------\n",
      "training loss: 29.902418\n",
      "Validation loss: 34.175592 \n",
      "\n",
      "Epoch 75\n",
      "----------------------------\n",
      "training loss: 30.256510\n",
      "Validation loss: 33.830881 \n",
      "\n",
      "Epoch 76\n",
      "----------------------------\n",
      "training loss: 29.589616\n",
      "Validation loss: 33.820910 \n",
      "\n",
      "Epoch 77\n",
      "----------------------------\n",
      "training loss: 29.445894\n",
      "Validation loss: 34.527657 \n",
      "\n",
      "Epoch 78\n",
      "----------------------------\n",
      "training loss: 29.857845\n",
      "Validation loss: 34.321957 \n",
      "\n",
      "Epoch 79\n",
      "----------------------------\n",
      "training loss: 29.540239\n",
      "Validation loss: 33.625490 \n",
      "\n",
      "Epoch 80\n",
      "----------------------------\n",
      "training loss: 29.535868\n",
      "Validation loss: 33.932916 \n",
      "\n",
      "Epoch 81\n",
      "----------------------------\n",
      "training loss: 29.350179\n",
      "Validation loss: 34.381731 \n",
      "\n",
      "Epoch 82\n",
      "----------------------------\n",
      "training loss: 29.628140\n",
      "Validation loss: 33.520591 \n",
      "\n",
      "Epoch 83\n",
      "----------------------------\n",
      "training loss: 29.428264\n",
      "Validation loss: 33.552964 \n",
      "\n",
      "Epoch 84\n",
      "----------------------------\n",
      "training loss: 29.568238\n",
      "Validation loss: 33.879141 \n",
      "\n",
      "Epoch 85\n",
      "----------------------------\n",
      "training loss: 29.642941\n",
      "Validation loss: 33.955431 \n",
      "\n",
      "Epoch 86\n",
      "----------------------------\n",
      "training loss: 29.030898\n",
      "Validation loss: 33.768864 \n",
      "\n",
      "Epoch 87\n",
      "----------------------------\n",
      "training loss: 29.489143\n",
      "Validation loss: 34.724148 \n",
      "\n",
      "Epoch 88\n",
      "----------------------------\n",
      "training loss: 29.527734\n",
      "Validation loss: 34.303822 \n",
      "\n",
      "Epoch 89\n",
      "----------------------------\n",
      "training loss: 29.400595\n",
      "Validation loss: 34.211640 \n",
      "\n",
      "Epoch 90\n",
      "----------------------------\n",
      "training loss: 28.341218\n",
      "Validation loss: 32.725582 \n",
      "\n",
      "Epoch 91\n",
      "----------------------------\n",
      "training loss: 28.889089\n",
      "Validation loss: 33.501319 \n",
      "\n",
      "Epoch 92\n",
      "----------------------------\n",
      "training loss: 28.449292\n",
      "Validation loss: 33.148099 \n",
      "\n",
      "Epoch 93\n",
      "----------------------------\n",
      "training loss: 28.961442\n",
      "Validation loss: 34.362082 \n",
      "\n",
      "Epoch 94\n",
      "----------------------------\n",
      "training loss: 28.488575\n",
      "Validation loss: 32.778766 \n",
      "\n",
      "Epoch 95\n",
      "----------------------------\n",
      "training loss: 28.482324\n",
      "Validation loss: 32.982853 \n",
      "\n",
      "Epoch 96\n",
      "----------------------------\n",
      "training loss: 27.845256\n",
      "Validation loss: 32.675574 \n",
      "\n",
      "Epoch 97\n",
      "----------------------------\n",
      "training loss: 29.357024\n",
      "Validation loss: 33.517886 \n",
      "\n",
      "Epoch 98\n",
      "----------------------------\n",
      "training loss: 28.264266\n",
      "Validation loss: 33.285511 \n",
      "\n",
      "Epoch 99\n",
      "----------------------------\n",
      "training loss: 27.770254\n",
      "Validation loss: 34.978380 \n",
      "\n",
      "Epoch 100\n",
      "----------------------------\n",
      "training loss: 28.459121\n",
      "Validation loss: 35.182462 \n",
      "\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "training loss: 179.814789\n",
      "Validation loss: 162.015797 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "training loss: 152.963117\n",
      "Validation loss: 133.852946 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "training loss: 103.175982\n",
      "Validation loss: 77.011369 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "training loss: 64.046733\n",
      "Validation loss: 57.819518 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "training loss: 51.517719\n",
      "Validation loss: 46.227742 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "training loss: 44.053933\n",
      "Validation loss: 41.558811 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "training loss: 41.600626\n",
      "Validation loss: 39.174577 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "training loss: 40.125134\n",
      "Validation loss: 40.287562 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "training loss: 40.253437\n",
      "Validation loss: 37.884025 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "training loss: 39.024434\n",
      "Validation loss: 38.672159 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "training loss: 38.684779\n",
      "Validation loss: 38.185509 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "training loss: 38.506913\n",
      "Validation loss: 36.557202 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "training loss: 37.410548\n",
      "Validation loss: 38.479538 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "training loss: 37.455437\n",
      "Validation loss: 36.980962 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "training loss: 36.301546\n",
      "Validation loss: 38.652241 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "training loss: 35.981669\n",
      "Validation loss: 37.666758 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "training loss: 35.324558\n",
      "Validation loss: 36.465325 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "training loss: 35.992900\n",
      "Validation loss: 35.586266 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "training loss: 35.585553\n",
      "Validation loss: 36.116001 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "training loss: 35.415317\n",
      "Validation loss: 35.711046 \n",
      "\n",
      "Epoch 21\n",
      "----------------------------\n",
      "training loss: 33.934658\n",
      "Validation loss: 33.913335 \n",
      "\n",
      "Epoch 22\n",
      "----------------------------\n",
      "training loss: 34.031395\n",
      "Validation loss: 36.785922 \n",
      "\n",
      "Epoch 23\n",
      "----------------------------\n",
      "training loss: 34.300584\n",
      "Validation loss: 35.732152 \n",
      "\n",
      "Epoch 24\n",
      "----------------------------\n",
      "training loss: 34.000697\n",
      "Validation loss: 36.156470 \n",
      "\n",
      "Epoch 25\n",
      "----------------------------\n",
      "training loss: 33.883986\n",
      "Validation loss: 38.776041 \n",
      "\n",
      "Epoch 26\n",
      "----------------------------\n",
      "training loss: 34.088438\n",
      "Validation loss: 35.693744 \n",
      "\n",
      "Epoch 27\n",
      "----------------------------\n",
      "training loss: 33.093142\n",
      "Validation loss: 35.395745 \n",
      "\n",
      "Epoch 28\n",
      "----------------------------\n",
      "training loss: 32.642414\n",
      "Validation loss: 38.737415 \n",
      "\n",
      "Epoch 29\n",
      "----------------------------\n",
      "training loss: 33.452340\n",
      "Validation loss: 35.068590 \n",
      "\n",
      "Epoch 30\n",
      "----------------------------\n",
      "training loss: 33.594625\n",
      "Validation loss: 37.616403 \n",
      "\n",
      "Epoch 31\n",
      "----------------------------\n",
      "training loss: 33.027639\n",
      "Validation loss: 36.034229 \n",
      "\n",
      "Epoch 32\n",
      "----------------------------\n",
      "training loss: 31.552075\n",
      "Validation loss: 32.573109 \n",
      "\n",
      "Epoch 33\n",
      "----------------------------\n",
      "training loss: 31.845730\n",
      "Validation loss: 33.456362 \n",
      "\n",
      "Epoch 34\n",
      "----------------------------\n",
      "training loss: 31.920947\n",
      "Validation loss: 35.274514 \n",
      "\n",
      "Epoch 35\n",
      "----------------------------\n",
      "training loss: 32.400762\n",
      "Validation loss: 33.244994 \n",
      "\n",
      "Epoch 36\n",
      "----------------------------\n",
      "training loss: 31.961558\n",
      "Validation loss: 33.563701 \n",
      "\n",
      "Epoch 37\n",
      "----------------------------\n",
      "training loss: 32.464182\n",
      "Validation loss: 34.221233 \n",
      "\n",
      "Epoch 38\n",
      "----------------------------\n",
      "training loss: 31.559805\n",
      "Validation loss: 32.718533 \n",
      "\n",
      "Epoch 39\n",
      "----------------------------\n",
      "training loss: 31.838248\n",
      "Validation loss: 34.267183 \n",
      "\n",
      "Epoch 40\n",
      "----------------------------\n",
      "training loss: 30.926120\n",
      "Validation loss: 32.983256 \n",
      "\n",
      "Epoch 41\n",
      "----------------------------\n",
      "training loss: 32.096153\n",
      "Validation loss: 34.681265 \n",
      "\n",
      "Epoch 42\n",
      "----------------------------\n",
      "training loss: 31.717916\n",
      "Validation loss: 34.675388 \n",
      "\n",
      "Epoch 43\n",
      "----------------------------\n",
      "training loss: 31.220007\n",
      "Validation loss: 36.027652 \n",
      "\n",
      "Epoch 44\n",
      "----------------------------\n",
      "training loss: 30.670010\n",
      "Validation loss: 32.745676 \n",
      "\n",
      "Epoch 45\n",
      "----------------------------\n",
      "training loss: 32.371930\n",
      "Validation loss: 32.912519 \n",
      "\n",
      "Epoch 46\n",
      "----------------------------\n",
      "training loss: 30.097624\n",
      "Validation loss: 32.629358 \n",
      "\n",
      "Epoch 47\n",
      "----------------------------\n",
      "training loss: 29.930689\n",
      "Validation loss: 33.464019 \n",
      "\n",
      "Epoch 48\n",
      "----------------------------\n",
      "training loss: 30.399189\n",
      "Validation loss: 33.066073 \n",
      "\n",
      "Epoch 49\n",
      "----------------------------\n",
      "training loss: 30.469985\n",
      "Validation loss: 33.634205 \n",
      "\n",
      "Epoch 50\n",
      "----------------------------\n",
      "training loss: 30.026454\n",
      "Validation loss: 33.238141 \n",
      "\n",
      "Epoch 51\n",
      "----------------------------\n",
      "training loss: 30.017699\n",
      "Validation loss: 31.671877 \n",
      "\n",
      "Epoch 52\n",
      "----------------------------\n",
      "training loss: 29.937554\n",
      "Validation loss: 32.466925 \n",
      "\n",
      "Epoch 53\n",
      "----------------------------\n",
      "training loss: 30.236958\n",
      "Validation loss: 32.454467 \n",
      "\n",
      "Epoch 54\n",
      "----------------------------\n",
      "training loss: 29.817771\n",
      "Validation loss: 33.054537 \n",
      "\n",
      "Epoch 55\n",
      "----------------------------\n",
      "training loss: 30.422633\n",
      "Validation loss: 33.791148 \n",
      "\n",
      "Epoch 56\n",
      "----------------------------\n",
      "training loss: 29.692701\n",
      "Validation loss: 32.470218 \n",
      "\n",
      "Epoch 57\n",
      "----------------------------\n",
      "training loss: 29.347351\n",
      "Validation loss: 33.352437 \n",
      "\n",
      "Epoch 58\n",
      "----------------------------\n",
      "training loss: 29.801362\n",
      "Validation loss: 33.699839 \n",
      "\n",
      "Epoch 59\n",
      "----------------------------\n",
      "training loss: 29.800037\n",
      "Validation loss: 33.093998 \n",
      "\n",
      "Epoch 60\n",
      "----------------------------\n",
      "training loss: 28.819935\n",
      "Validation loss: 32.800217 \n",
      "\n",
      "Epoch 61\n",
      "----------------------------\n",
      "training loss: 29.519838\n",
      "Validation loss: 35.015041 \n",
      "\n",
      "Epoch 62\n",
      "----------------------------\n",
      "training loss: 29.496447\n",
      "Validation loss: 32.495227 \n",
      "\n",
      "Epoch 63\n",
      "----------------------------\n",
      "training loss: 28.992792\n",
      "Validation loss: 32.238230 \n",
      "\n",
      "Epoch 64\n",
      "----------------------------\n",
      "training loss: 28.498902\n",
      "Validation loss: 32.410759 \n",
      "\n",
      "Epoch 65\n",
      "----------------------------\n",
      "training loss: 28.989720\n",
      "Validation loss: 34.200065 \n",
      "\n",
      "Epoch 66\n",
      "----------------------------\n",
      "training loss: 28.486377\n",
      "Validation loss: 32.488657 \n",
      "\n",
      "Epoch 67\n",
      "----------------------------\n",
      "training loss: 28.880803\n",
      "Validation loss: 33.261872 \n",
      "\n",
      "Epoch 68\n",
      "----------------------------\n",
      "training loss: 28.945257\n",
      "Validation loss: 32.407669 \n",
      "\n",
      "Epoch 69\n",
      "----------------------------\n",
      "training loss: 28.569580\n",
      "Validation loss: 32.591928 \n",
      "\n",
      "Epoch 70\n",
      "----------------------------\n",
      "training loss: 27.970994\n",
      "Validation loss: 31.524437 \n",
      "\n",
      "Epoch 71\n",
      "----------------------------\n",
      "training loss: 28.754347\n",
      "Validation loss: 32.230788 \n",
      "\n",
      "Epoch 72\n",
      "----------------------------\n",
      "training loss: 28.243710\n",
      "Validation loss: 31.653025 \n",
      "\n",
      "Epoch 73\n",
      "----------------------------\n",
      "training loss: 28.710516\n",
      "Validation loss: 31.532028 \n",
      "\n",
      "Epoch 74\n",
      "----------------------------\n",
      "training loss: 28.654628\n",
      "Validation loss: 33.941029 \n",
      "\n",
      "Epoch 75\n",
      "----------------------------\n",
      "training loss: 28.651073\n",
      "Validation loss: 32.027904 \n",
      "\n",
      "Epoch 76\n",
      "----------------------------\n",
      "training loss: 28.603537\n",
      "Validation loss: 31.825383 \n",
      "\n",
      "Epoch 77\n",
      "----------------------------\n",
      "training loss: 28.214455\n",
      "Validation loss: 33.046310 \n",
      "\n",
      "Epoch 78\n",
      "----------------------------\n",
      "training loss: 28.386199\n",
      "Validation loss: 32.151476 \n",
      "\n",
      "Epoch 79\n",
      "----------------------------\n",
      "training loss: 27.942976\n",
      "Validation loss: 32.959194 \n",
      "\n",
      "Epoch 80\n",
      "----------------------------\n",
      "training loss: 28.174531\n",
      "Validation loss: 32.381227 \n",
      "\n",
      "Epoch 81\n",
      "----------------------------\n",
      "training loss: 27.492383\n",
      "Validation loss: 36.230720 \n",
      "\n",
      "Epoch 82\n",
      "----------------------------\n",
      "training loss: 28.203449\n",
      "Validation loss: 36.716199 \n",
      "\n",
      "Epoch 83\n",
      "----------------------------\n",
      "training loss: 27.749233\n",
      "Validation loss: 31.421276 \n",
      "\n",
      "Epoch 84\n",
      "----------------------------\n",
      "training loss: 27.356469\n",
      "Validation loss: 34.069469 \n",
      "\n",
      "Epoch 85\n",
      "----------------------------\n",
      "training loss: 27.163045\n",
      "Validation loss: 34.462756 \n",
      "\n",
      "Epoch 86\n",
      "----------------------------\n",
      "training loss: 27.021940\n",
      "Validation loss: 31.728850 \n",
      "\n",
      "Epoch 87\n",
      "----------------------------\n",
      "training loss: 27.198728\n",
      "Validation loss: 33.151766 \n",
      "\n",
      "Epoch 88\n",
      "----------------------------\n",
      "training loss: 27.736758\n",
      "Validation loss: 31.707999 \n",
      "\n",
      "Epoch 89\n",
      "----------------------------\n",
      "training loss: 27.174718\n",
      "Validation loss: 32.214329 \n",
      "\n",
      "Epoch 90\n",
      "----------------------------\n",
      "training loss: 26.825301\n",
      "Validation loss: 32.631424 \n",
      "\n",
      "Epoch 91\n",
      "----------------------------\n",
      "training loss: 27.521596\n",
      "Validation loss: 31.834579 \n",
      "\n",
      "Epoch 92\n",
      "----------------------------\n",
      "training loss: 27.843291\n",
      "Validation loss: 30.656347 \n",
      "\n",
      "Epoch 93\n",
      "----------------------------\n",
      "training loss: 26.918948\n",
      "Validation loss: 35.936626 \n",
      "\n",
      "Epoch 94\n",
      "----------------------------\n",
      "training loss: 26.829967\n",
      "Validation loss: 34.154204 \n",
      "\n",
      "Epoch 95\n",
      "----------------------------\n",
      "training loss: 26.695001\n",
      "Validation loss: 32.252604 \n",
      "\n",
      "Epoch 96\n",
      "----------------------------\n",
      "training loss: 27.413974\n",
      "Validation loss: 31.643517 \n",
      "\n",
      "Epoch 97\n",
      "----------------------------\n",
      "training loss: 26.486361\n",
      "Validation loss: 31.050588 \n",
      "\n",
      "Epoch 98\n",
      "----------------------------\n",
      "training loss: 26.331977\n",
      "Validation loss: 32.691241 \n",
      "\n",
      "Epoch 99\n",
      "----------------------------\n",
      "training loss: 26.186722\n",
      "Validation loss: 32.000838 \n",
      "\n",
      "Epoch 100\n",
      "----------------------------\n",
      "training loss: 26.133854\n",
      "Validation loss: 31.896394 \n",
      "\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "training loss: 177.653515\n",
      "Validation loss: 175.403308 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "training loss: 137.238611\n",
      "Validation loss: 110.774427 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "training loss: 70.600354\n",
      "Validation loss: 64.426549 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "training loss: 56.821686\n",
      "Validation loss: 57.884054 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "training loss: 55.585710\n",
      "Validation loss: 56.732762 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "training loss: 52.620829\n",
      "Validation loss: 58.276272 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "training loss: 52.566638\n",
      "Validation loss: 54.830533 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "training loss: 50.465774\n",
      "Validation loss: 55.606278 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "training loss: 49.956369\n",
      "Validation loss: 56.383505 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "training loss: 49.027953\n",
      "Validation loss: 55.689619 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "training loss: 47.800765\n",
      "Validation loss: 55.821990 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "training loss: 47.988460\n",
      "Validation loss: 53.298021 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "training loss: 46.943860\n",
      "Validation loss: 55.526556 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "training loss: 46.044876\n",
      "Validation loss: 51.721074 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "training loss: 45.191844\n",
      "Validation loss: 49.689730 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "training loss: 45.266831\n",
      "Validation loss: 48.507780 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "training loss: 43.361310\n",
      "Validation loss: 46.973073 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "training loss: 42.523628\n",
      "Validation loss: 46.670780 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "training loss: 41.598379\n",
      "Validation loss: 45.110205 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "training loss: 41.984810\n",
      "Validation loss: 46.846094 \n",
      "\n",
      "Epoch 21\n",
      "----------------------------\n",
      "training loss: 41.074053\n",
      "Validation loss: 44.770550 \n",
      "\n",
      "Epoch 22\n",
      "----------------------------\n",
      "training loss: 39.410010\n",
      "Validation loss: 43.457556 \n",
      "\n",
      "Epoch 23\n",
      "----------------------------\n",
      "training loss: 39.500906\n",
      "Validation loss: 44.358559 \n",
      "\n",
      "Epoch 24\n",
      "----------------------------\n",
      "training loss: 38.722947\n",
      "Validation loss: 43.163618 \n",
      "\n",
      "Epoch 25\n",
      "----------------------------\n",
      "training loss: 38.382040\n",
      "Validation loss: 43.297918 \n",
      "\n",
      "Epoch 26\n",
      "----------------------------\n",
      "training loss: 38.411991\n",
      "Validation loss: 43.598519 \n",
      "\n",
      "Epoch 27\n",
      "----------------------------\n",
      "training loss: 37.521075\n",
      "Validation loss: 41.072789 \n",
      "\n",
      "Epoch 28\n",
      "----------------------------\n",
      "training loss: 37.594965\n",
      "Validation loss: 42.799774 \n",
      "\n",
      "Epoch 29\n",
      "----------------------------\n",
      "training loss: 36.581876\n",
      "Validation loss: 41.497508 \n",
      "\n",
      "Epoch 30\n",
      "----------------------------\n",
      "training loss: 37.120394\n",
      "Validation loss: 39.601898 \n",
      "\n",
      "Epoch 31\n",
      "----------------------------\n",
      "training loss: 36.230337\n",
      "Validation loss: 40.933777 \n",
      "\n",
      "Epoch 32\n",
      "----------------------------\n",
      "training loss: 35.899117\n",
      "Validation loss: 41.261829 \n",
      "\n",
      "Epoch 33\n",
      "----------------------------\n",
      "training loss: 35.412061\n",
      "Validation loss: 38.987882 \n",
      "\n",
      "Epoch 34\n",
      "----------------------------\n",
      "training loss: 35.411883\n",
      "Validation loss: 39.719501 \n",
      "\n",
      "Epoch 35\n",
      "----------------------------\n",
      "training loss: 35.705886\n",
      "Validation loss: 40.823480 \n",
      "\n",
      "Epoch 36\n",
      "----------------------------\n",
      "training loss: 35.494771\n",
      "Validation loss: 39.192406 \n",
      "\n",
      "Epoch 37\n",
      "----------------------------\n",
      "training loss: 34.883540\n",
      "Validation loss: 41.053514 \n",
      "\n",
      "Epoch 38\n",
      "----------------------------\n",
      "training loss: 34.843163\n",
      "Validation loss: 41.797116 \n",
      "\n",
      "Epoch 39\n",
      "----------------------------\n",
      "training loss: 34.349630\n",
      "Validation loss: 39.442766 \n",
      "\n",
      "Epoch 40\n",
      "----------------------------\n",
      "training loss: 35.119816\n",
      "Validation loss: 38.969774 \n",
      "\n",
      "Epoch 41\n",
      "----------------------------\n",
      "training loss: 34.415866\n",
      "Validation loss: 39.331084 \n",
      "\n",
      "Epoch 42\n",
      "----------------------------\n",
      "training loss: 34.542073\n",
      "Validation loss: 39.856056 \n",
      "\n",
      "Epoch 43\n",
      "----------------------------\n",
      "training loss: 34.397770\n",
      "Validation loss: 39.672294 \n",
      "\n",
      "Epoch 44\n",
      "----------------------------\n",
      "training loss: 34.338535\n",
      "Validation loss: 38.043564 \n",
      "\n",
      "Epoch 45\n",
      "----------------------------\n",
      "training loss: 34.294480\n",
      "Validation loss: 39.540401 \n",
      "\n",
      "Epoch 46\n",
      "----------------------------\n",
      "training loss: 33.841914\n",
      "Validation loss: 38.158130 \n",
      "\n",
      "Epoch 47\n",
      "----------------------------\n",
      "training loss: 34.509581\n",
      "Validation loss: 39.984680 \n",
      "\n",
      "Epoch 48\n",
      "----------------------------\n",
      "training loss: 34.061928\n",
      "Validation loss: 40.956528 \n",
      "\n",
      "Epoch 49\n",
      "----------------------------\n",
      "training loss: 34.388784\n",
      "Validation loss: 42.013794 \n",
      "\n",
      "Epoch 50\n",
      "----------------------------\n",
      "training loss: 33.392374\n",
      "Validation loss: 38.912651 \n",
      "\n",
      "Epoch 51\n",
      "----------------------------\n",
      "training loss: 34.115090\n",
      "Validation loss: 39.558420 \n",
      "\n",
      "Epoch 52\n",
      "----------------------------\n",
      "training loss: 33.354639\n",
      "Validation loss: 38.962878 \n",
      "\n",
      "Epoch 53\n",
      "----------------------------\n",
      "training loss: 33.266136\n",
      "Validation loss: 37.984817 \n",
      "\n",
      "Epoch 54\n",
      "----------------------------\n",
      "training loss: 33.643578\n",
      "Validation loss: 39.132556 \n",
      "\n",
      "Epoch 55\n",
      "----------------------------\n",
      "training loss: 32.780346\n",
      "Validation loss: 38.749145 \n",
      "\n",
      "Epoch 56\n",
      "----------------------------\n",
      "training loss: 33.170615\n",
      "Validation loss: 38.907143 \n",
      "\n",
      "Epoch 57\n",
      "----------------------------\n",
      "training loss: 33.176649\n",
      "Validation loss: 39.713537 \n",
      "\n",
      "Epoch 58\n",
      "----------------------------\n",
      "training loss: 34.085275\n",
      "Validation loss: 40.125479 \n",
      "\n",
      "Epoch 59\n",
      "----------------------------\n",
      "training loss: 32.561796\n",
      "Validation loss: 40.353571 \n",
      "\n",
      "Epoch 60\n",
      "----------------------------\n",
      "training loss: 33.329108\n",
      "Validation loss: 43.053587 \n",
      "\n",
      "Epoch 61\n",
      "----------------------------\n",
      "training loss: 32.479437\n",
      "Validation loss: 40.352099 \n",
      "\n",
      "Epoch 62\n",
      "----------------------------\n",
      "training loss: 33.040791\n",
      "Validation loss: 39.621062 \n",
      "\n",
      "Epoch 63\n",
      "----------------------------\n",
      "training loss: 32.530807\n",
      "Validation loss: 39.345031 \n",
      "\n",
      "Epoch 64\n",
      "----------------------------\n",
      "training loss: 32.219436\n",
      "Validation loss: 38.494495 \n",
      "\n",
      "Epoch 65\n",
      "----------------------------\n",
      "training loss: 32.607612\n",
      "Validation loss: 39.906246 \n",
      "\n",
      "Epoch 66\n",
      "----------------------------\n",
      "training loss: 32.411054\n",
      "Validation loss: 38.471542 \n",
      "\n",
      "Epoch 67\n",
      "----------------------------\n",
      "training loss: 32.363142\n",
      "Validation loss: 38.842109 \n",
      "\n",
      "Epoch 68\n",
      "----------------------------\n",
      "training loss: 32.425475\n",
      "Validation loss: 38.713471 \n",
      "\n",
      "Epoch 69\n",
      "----------------------------\n",
      "training loss: 31.842241\n",
      "Validation loss: 38.990082 \n",
      "\n",
      "Epoch 70\n",
      "----------------------------\n",
      "training loss: 32.571215\n",
      "Validation loss: 37.990952 \n",
      "\n",
      "Epoch 71\n",
      "----------------------------\n",
      "training loss: 32.212416\n",
      "Validation loss: 38.434438 \n",
      "\n",
      "Epoch 72\n",
      "----------------------------\n",
      "training loss: 32.208989\n",
      "Validation loss: 38.775456 \n",
      "\n",
      "Epoch 73\n",
      "----------------------------\n",
      "training loss: 31.327588\n",
      "Validation loss: 40.959224 \n",
      "\n",
      "Epoch 74\n",
      "----------------------------\n",
      "training loss: 31.925151\n",
      "Validation loss: 36.646152 \n",
      "\n",
      "Epoch 75\n",
      "----------------------------\n",
      "training loss: 31.629419\n",
      "Validation loss: 38.138856 \n",
      "\n",
      "Epoch 76\n",
      "----------------------------\n",
      "training loss: 31.593664\n",
      "Validation loss: 40.515107 \n",
      "\n",
      "Epoch 77\n",
      "----------------------------\n",
      "training loss: 31.682432\n",
      "Validation loss: 37.521337 \n",
      "\n",
      "Epoch 78\n",
      "----------------------------\n",
      "training loss: 31.099113\n",
      "Validation loss: 39.193179 \n",
      "\n",
      "Epoch 79\n",
      "----------------------------\n",
      "training loss: 32.527701\n",
      "Validation loss: 40.064116 \n",
      "\n",
      "Epoch 80\n",
      "----------------------------\n",
      "training loss: 31.534222\n",
      "Validation loss: 38.011434 \n",
      "\n",
      "Epoch 81\n",
      "----------------------------\n",
      "training loss: 31.252044\n",
      "Validation loss: 38.150320 \n",
      "\n",
      "Epoch 82\n",
      "----------------------------\n",
      "training loss: 31.415667\n",
      "Validation loss: 36.302365 \n",
      "\n",
      "Epoch 83\n",
      "----------------------------\n",
      "training loss: 30.855030\n",
      "Validation loss: 38.699001 \n",
      "\n",
      "Epoch 84\n",
      "----------------------------\n",
      "training loss: 31.310214\n",
      "Validation loss: 39.100151 \n",
      "\n",
      "Epoch 85\n",
      "----------------------------\n",
      "training loss: 30.763974\n",
      "Validation loss: 38.445871 \n",
      "\n",
      "Epoch 86\n",
      "----------------------------\n",
      "training loss: 31.693119\n",
      "Validation loss: 39.924204 \n",
      "\n",
      "Epoch 87\n",
      "----------------------------\n",
      "training loss: 31.117836\n",
      "Validation loss: 38.893202 \n",
      "\n",
      "Epoch 88\n",
      "----------------------------\n",
      "training loss: 30.526063\n",
      "Validation loss: 39.822142 \n",
      "\n",
      "Epoch 89\n",
      "----------------------------\n",
      "training loss: 29.808624\n",
      "Validation loss: 36.227841 \n",
      "\n",
      "Epoch 90\n",
      "----------------------------\n",
      "training loss: 30.769115\n",
      "Validation loss: 38.669842 \n",
      "\n",
      "Epoch 91\n",
      "----------------------------\n",
      "training loss: 30.680887\n",
      "Validation loss: 38.136915 \n",
      "\n",
      "Epoch 92\n",
      "----------------------------\n",
      "training loss: 31.205595\n",
      "Validation loss: 35.655510 \n",
      "\n",
      "Epoch 93\n",
      "----------------------------\n",
      "training loss: 30.868353\n",
      "Validation loss: 38.906234 \n",
      "\n",
      "Epoch 94\n",
      "----------------------------\n",
      "training loss: 29.664697\n",
      "Validation loss: 39.133969 \n",
      "\n",
      "Epoch 95\n",
      "----------------------------\n",
      "training loss: 30.213455\n",
      "Validation loss: 38.858763 \n",
      "\n",
      "Epoch 96\n",
      "----------------------------\n",
      "training loss: 30.502612\n",
      "Validation loss: 39.272603 \n",
      "\n",
      "Epoch 97\n",
      "----------------------------\n",
      "training loss: 30.311702\n",
      "Validation loss: 38.593022 \n",
      "\n",
      "Epoch 98\n",
      "----------------------------\n",
      "training loss: 30.139977\n",
      "Validation loss: 39.205438 \n",
      "\n",
      "Epoch 99\n",
      "----------------------------\n",
      "training loss: 30.779156\n",
      "Validation loss: 36.917730 \n",
      "\n",
      "Epoch 100\n",
      "----------------------------\n",
      "training loss: 30.414001\n",
      "Validation loss: 40.278579 \n",
      "\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "training loss: 181.969964\n",
      "Validation loss: 175.285097 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "training loss: 154.214147\n",
      "Validation loss: 138.600163 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "training loss: 98.686226\n",
      "Validation loss: 77.987927 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "training loss: 71.298580\n",
      "Validation loss: 61.857541 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "training loss: 59.801963\n",
      "Validation loss: 50.005834 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "training loss: 51.965990\n",
      "Validation loss: 44.811456 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "training loss: 47.254782\n",
      "Validation loss: 43.760514 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "training loss: 44.962414\n",
      "Validation loss: 40.328295 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "training loss: 43.324263\n",
      "Validation loss: 40.214129 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "training loss: 41.161227\n",
      "Validation loss: 38.338809 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "training loss: 40.992866\n",
      "Validation loss: 37.164218 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "training loss: 39.402918\n",
      "Validation loss: 35.767092 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "training loss: 38.740203\n",
      "Validation loss: 35.580879 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "training loss: 38.016805\n",
      "Validation loss: 35.367767 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "training loss: 37.346995\n",
      "Validation loss: 35.197114 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "training loss: 37.427563\n",
      "Validation loss: 35.745060 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "training loss: 37.335084\n",
      "Validation loss: 35.268118 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "training loss: 35.909608\n",
      "Validation loss: 34.707216 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "training loss: 36.107980\n",
      "Validation loss: 33.614461 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "training loss: 35.807950\n",
      "Validation loss: 34.327428 \n",
      "\n",
      "Epoch 21\n",
      "----------------------------\n",
      "training loss: 36.102706\n",
      "Validation loss: 33.422959 \n",
      "\n",
      "Epoch 22\n",
      "----------------------------\n",
      "training loss: 35.067503\n",
      "Validation loss: 34.222017 \n",
      "\n",
      "Epoch 23\n",
      "----------------------------\n",
      "training loss: 35.814258\n",
      "Validation loss: 34.060041 \n",
      "\n",
      "Epoch 24\n",
      "----------------------------\n",
      "training loss: 34.423252\n",
      "Validation loss: 33.730595 \n",
      "\n",
      "Epoch 25\n",
      "----------------------------\n",
      "training loss: 34.631854\n",
      "Validation loss: 34.434222 \n",
      "\n",
      "Epoch 26\n",
      "----------------------------\n",
      "training loss: 34.998230\n",
      "Validation loss: 32.879894 \n",
      "\n",
      "Epoch 27\n",
      "----------------------------\n",
      "training loss: 34.945598\n",
      "Validation loss: 32.592677 \n",
      "\n",
      "Epoch 28\n",
      "----------------------------\n",
      "training loss: 34.238949\n",
      "Validation loss: 33.167687 \n",
      "\n",
      "Epoch 29\n",
      "----------------------------\n",
      "training loss: 34.470776\n",
      "Validation loss: 32.564863 \n",
      "\n",
      "Epoch 30\n",
      "----------------------------\n",
      "training loss: 34.888099\n",
      "Validation loss: 32.240376 \n",
      "\n",
      "Epoch 31\n",
      "----------------------------\n",
      "training loss: 33.749522\n",
      "Validation loss: 35.014601 \n",
      "\n",
      "Epoch 32\n",
      "----------------------------\n",
      "training loss: 33.556175\n",
      "Validation loss: 33.281259 \n",
      "\n",
      "Epoch 33\n",
      "----------------------------\n",
      "training loss: 33.862987\n",
      "Validation loss: 31.660437 \n",
      "\n",
      "Epoch 34\n",
      "----------------------------\n",
      "training loss: 33.186207\n",
      "Validation loss: 31.542436 \n",
      "\n",
      "Epoch 35\n",
      "----------------------------\n",
      "training loss: 32.476053\n",
      "Validation loss: 33.745553 \n",
      "\n",
      "Epoch 36\n",
      "----------------------------\n",
      "training loss: 32.575656\n",
      "Validation loss: 32.304147 \n",
      "\n",
      "Epoch 37\n",
      "----------------------------\n",
      "training loss: 33.015374\n",
      "Validation loss: 32.416294 \n",
      "\n",
      "Epoch 38\n",
      "----------------------------\n",
      "training loss: 33.661283\n",
      "Validation loss: 32.900707 \n",
      "\n",
      "Epoch 39\n",
      "----------------------------\n",
      "training loss: 32.910448\n",
      "Validation loss: 32.574552 \n",
      "\n",
      "Epoch 40\n",
      "----------------------------\n",
      "training loss: 32.952033\n",
      "Validation loss: 32.518077 \n",
      "\n",
      "Epoch 41\n",
      "----------------------------\n",
      "training loss: 32.639825\n",
      "Validation loss: 31.994240 \n",
      "\n",
      "Epoch 42\n",
      "----------------------------\n",
      "training loss: 32.349468\n",
      "Validation loss: 31.669247 \n",
      "\n",
      "Epoch 43\n",
      "----------------------------\n",
      "training loss: 32.250782\n",
      "Validation loss: 32.151903 \n",
      "\n",
      "Epoch 44\n",
      "----------------------------\n",
      "training loss: 32.388370\n",
      "Validation loss: 32.161287 \n",
      "\n",
      "Epoch 45\n",
      "----------------------------\n",
      "training loss: 32.510470\n",
      "Validation loss: 31.486383 \n",
      "\n",
      "Epoch 46\n",
      "----------------------------\n",
      "training loss: 31.858690\n",
      "Validation loss: 31.371129 \n",
      "\n",
      "Epoch 47\n",
      "----------------------------\n",
      "training loss: 31.871661\n",
      "Validation loss: 32.143931 \n",
      "\n",
      "Epoch 48\n",
      "----------------------------\n",
      "training loss: 31.940589\n",
      "Validation loss: 30.959956 \n",
      "\n",
      "Epoch 49\n",
      "----------------------------\n",
      "training loss: 31.553498\n",
      "Validation loss: 31.838410 \n",
      "\n",
      "Epoch 50\n",
      "----------------------------\n",
      "training loss: 32.363788\n",
      "Validation loss: 32.615833 \n",
      "\n",
      "Epoch 51\n",
      "----------------------------\n",
      "training loss: 30.687897\n",
      "Validation loss: 31.400642 \n",
      "\n",
      "Epoch 52\n",
      "----------------------------\n",
      "training loss: 31.265853\n",
      "Validation loss: 31.999326 \n",
      "\n",
      "Epoch 53\n",
      "----------------------------\n",
      "training loss: 31.035886\n",
      "Validation loss: 31.674711 \n",
      "\n",
      "Epoch 54\n",
      "----------------------------\n",
      "training loss: 31.019114\n",
      "Validation loss: 31.716166 \n",
      "\n",
      "Epoch 55\n",
      "----------------------------\n",
      "training loss: 31.197444\n",
      "Validation loss: 31.747713 \n",
      "\n",
      "Epoch 56\n",
      "----------------------------\n",
      "training loss: 31.203320\n",
      "Validation loss: 31.281438 \n",
      "\n",
      "Epoch 57\n",
      "----------------------------\n",
      "training loss: 30.495233\n",
      "Validation loss: 30.999860 \n",
      "\n",
      "Epoch 58\n",
      "----------------------------\n",
      "training loss: 31.261068\n",
      "Validation loss: 31.100642 \n",
      "\n",
      "Epoch 59\n",
      "----------------------------\n",
      "training loss: 31.426904\n",
      "Validation loss: 31.769824 \n",
      "\n",
      "Epoch 60\n",
      "----------------------------\n",
      "training loss: 30.901470\n",
      "Validation loss: 31.498445 \n",
      "\n",
      "Epoch 61\n",
      "----------------------------\n",
      "training loss: 30.376036\n",
      "Validation loss: 32.029118 \n",
      "\n",
      "Epoch 62\n",
      "----------------------------\n",
      "training loss: 30.862080\n",
      "Validation loss: 31.595980 \n",
      "\n",
      "Epoch 63\n",
      "----------------------------\n",
      "training loss: 31.123969\n",
      "Validation loss: 31.460918 \n",
      "\n",
      "Epoch 64\n",
      "----------------------------\n",
      "training loss: 30.643611\n",
      "Validation loss: 32.375668 \n",
      "\n",
      "Epoch 65\n",
      "----------------------------\n",
      "training loss: 30.083808\n",
      "Validation loss: 32.927505 \n",
      "\n",
      "Epoch 66\n",
      "----------------------------\n",
      "training loss: 30.078600\n",
      "Validation loss: 32.058522 \n",
      "\n",
      "Epoch 67\n",
      "----------------------------\n",
      "training loss: 30.339062\n",
      "Validation loss: 30.797771 \n",
      "\n",
      "Epoch 68\n",
      "----------------------------\n",
      "training loss: 30.379201\n",
      "Validation loss: 32.353091 \n",
      "\n",
      "Epoch 69\n",
      "----------------------------\n",
      "training loss: 30.177886\n",
      "Validation loss: 32.480018 \n",
      "\n",
      "Epoch 70\n",
      "----------------------------\n",
      "training loss: 30.223091\n",
      "Validation loss: 32.121233 \n",
      "\n",
      "Epoch 71\n",
      "----------------------------\n",
      "training loss: 30.496787\n",
      "Validation loss: 30.660924 \n",
      "\n",
      "Epoch 72\n",
      "----------------------------\n",
      "training loss: 30.186167\n",
      "Validation loss: 31.352487 \n",
      "\n",
      "Epoch 73\n",
      "----------------------------\n",
      "training loss: 29.540256\n",
      "Validation loss: 31.132807 \n",
      "\n",
      "Epoch 74\n",
      "----------------------------\n",
      "training loss: 29.497874\n",
      "Validation loss: 31.941419 \n",
      "\n",
      "Epoch 75\n",
      "----------------------------\n",
      "training loss: 30.245603\n",
      "Validation loss: 30.891875 \n",
      "\n",
      "Epoch 76\n",
      "----------------------------\n",
      "training loss: 29.598023\n",
      "Validation loss: 33.638178 \n",
      "\n",
      "Epoch 77\n",
      "----------------------------\n",
      "training loss: 29.878860\n",
      "Validation loss: 30.856435 \n",
      "\n",
      "Epoch 78\n",
      "----------------------------\n",
      "training loss: 29.818369\n",
      "Validation loss: 30.941456 \n",
      "\n",
      "Epoch 79\n",
      "----------------------------\n",
      "training loss: 28.944342\n",
      "Validation loss: 32.338441 \n",
      "\n",
      "Epoch 80\n",
      "----------------------------\n",
      "training loss: 29.477613\n",
      "Validation loss: 32.180834 \n",
      "\n",
      "Epoch 81\n",
      "----------------------------\n",
      "training loss: 29.915019\n",
      "Validation loss: 31.042489 \n",
      "\n",
      "Epoch 82\n",
      "----------------------------\n",
      "training loss: 29.705443\n",
      "Validation loss: 31.905264 \n",
      "\n",
      "Epoch 83\n",
      "----------------------------\n",
      "training loss: 29.704425\n",
      "Validation loss: 30.826422 \n",
      "\n",
      "Epoch 84\n",
      "----------------------------\n",
      "training loss: 29.316807\n",
      "Validation loss: 31.235660 \n",
      "\n",
      "Epoch 85\n",
      "----------------------------\n",
      "training loss: 29.105582\n",
      "Validation loss: 30.599390 \n",
      "\n",
      "Epoch 86\n",
      "----------------------------\n",
      "training loss: 29.111351\n",
      "Validation loss: 30.959735 \n",
      "\n",
      "Epoch 87\n",
      "----------------------------\n",
      "training loss: 29.034855\n",
      "Validation loss: 31.414565 \n",
      "\n",
      "Epoch 88\n",
      "----------------------------\n",
      "training loss: 28.691079\n",
      "Validation loss: 31.811461 \n",
      "\n",
      "Epoch 89\n",
      "----------------------------\n",
      "training loss: 29.122919\n",
      "Validation loss: 31.426694 \n",
      "\n",
      "Epoch 90\n",
      "----------------------------\n",
      "training loss: 28.691035\n",
      "Validation loss: 31.891920 \n",
      "\n",
      "Epoch 91\n",
      "----------------------------\n",
      "training loss: 29.139906\n",
      "Validation loss: 31.024938 \n",
      "\n",
      "Epoch 92\n",
      "----------------------------\n",
      "training loss: 29.580912\n",
      "Validation loss: 31.812323 \n",
      "\n",
      "Epoch 93\n",
      "----------------------------\n",
      "training loss: 28.698559\n",
      "Validation loss: 31.252221 \n",
      "\n",
      "Epoch 94\n",
      "----------------------------\n",
      "training loss: 29.412479\n",
      "Validation loss: 31.668388 \n",
      "\n",
      "Epoch 95\n",
      "----------------------------\n",
      "training loss: 28.949799\n",
      "Validation loss: 30.582290 \n",
      "\n",
      "Epoch 96\n",
      "----------------------------\n",
      "training loss: 28.551766\n",
      "Validation loss: 30.520044 \n",
      "\n",
      "Epoch 97\n",
      "----------------------------\n",
      "training loss: 27.851909\n",
      "Validation loss: 30.872570 \n",
      "\n",
      "Epoch 98\n",
      "----------------------------\n",
      "training loss: 28.568088\n",
      "Validation loss: 31.522594 \n",
      "\n",
      "Epoch 99\n",
      "----------------------------\n",
      "training loss: 28.666543\n",
      "Validation loss: 31.190095 \n",
      "\n",
      "Epoch 100\n",
      "----------------------------\n",
      "training loss: 28.453600\n",
      "Validation loss: 32.179251 \n",
      "\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "Epoch 1\n",
      "----------------------------\n",
      "training loss: 180.192029\n",
      "Validation loss: 173.267002 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------\n",
      "training loss: 156.217046\n",
      "Validation loss: 147.769573 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------\n",
      "training loss: 117.404714\n",
      "Validation loss: 84.846917 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------\n",
      "training loss: 66.021997\n",
      "Validation loss: 56.228927 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------\n",
      "training loss: 51.010838\n",
      "Validation loss: 47.076799 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------\n",
      "training loss: 45.426491\n",
      "Validation loss: 46.350929 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------\n",
      "training loss: 43.391408\n",
      "Validation loss: 44.646322 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------\n",
      "training loss: 41.730252\n",
      "Validation loss: 44.217973 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------\n",
      "training loss: 40.013522\n",
      "Validation loss: 44.600145 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------\n",
      "training loss: 38.886536\n",
      "Validation loss: 42.271103 \n",
      "\n",
      "Epoch 11\n",
      "----------------------------\n",
      "training loss: 38.328499\n",
      "Validation loss: 42.130572 \n",
      "\n",
      "Epoch 12\n",
      "----------------------------\n",
      "training loss: 37.913154\n",
      "Validation loss: 42.538260 \n",
      "\n",
      "Epoch 13\n",
      "----------------------------\n",
      "training loss: 36.771806\n",
      "Validation loss: 41.391079 \n",
      "\n",
      "Epoch 14\n",
      "----------------------------\n",
      "training loss: 35.938910\n",
      "Validation loss: 39.542840 \n",
      "\n",
      "Epoch 15\n",
      "----------------------------\n",
      "training loss: 36.069942\n",
      "Validation loss: 43.652160 \n",
      "\n",
      "Epoch 16\n",
      "----------------------------\n",
      "training loss: 35.937900\n",
      "Validation loss: 40.556232 \n",
      "\n",
      "Epoch 17\n",
      "----------------------------\n",
      "training loss: 36.816064\n",
      "Validation loss: 40.180234 \n",
      "\n",
      "Epoch 18\n",
      "----------------------------\n",
      "training loss: 35.247123\n",
      "Validation loss: 40.882403 \n",
      "\n",
      "Epoch 19\n",
      "----------------------------\n",
      "training loss: 35.121354\n",
      "Validation loss: 38.303491 \n",
      "\n",
      "Epoch 20\n",
      "----------------------------\n",
      "training loss: 34.140876\n",
      "Validation loss: 38.626195 \n",
      "\n",
      "Epoch 21\n",
      "----------------------------\n",
      "training loss: 33.953668\n",
      "Validation loss: 38.354244 \n",
      "\n",
      "Epoch 22\n",
      "----------------------------\n",
      "training loss: 33.409922\n",
      "Validation loss: 43.716434 \n",
      "\n",
      "Epoch 23\n",
      "----------------------------\n",
      "training loss: 33.258560\n",
      "Validation loss: 40.367312 \n",
      "\n",
      "Epoch 24\n",
      "----------------------------\n",
      "training loss: 32.955730\n",
      "Validation loss: 39.116388 \n",
      "\n",
      "Epoch 25\n",
      "----------------------------\n",
      "training loss: 32.793097\n",
      "Validation loss: 38.568983 \n",
      "\n",
      "Epoch 26\n",
      "----------------------------\n",
      "training loss: 32.702201\n",
      "Validation loss: 38.252712 \n",
      "\n",
      "Epoch 27\n",
      "----------------------------\n",
      "training loss: 32.827593\n",
      "Validation loss: 38.067209 \n",
      "\n",
      "Epoch 28\n",
      "----------------------------\n",
      "training loss: 32.563104\n",
      "Validation loss: 38.931454 \n",
      "\n",
      "Epoch 29\n",
      "----------------------------\n",
      "training loss: 32.718194\n",
      "Validation loss: 37.131821 \n",
      "\n",
      "Epoch 30\n",
      "----------------------------\n",
      "training loss: 32.861713\n",
      "Validation loss: 38.332295 \n",
      "\n",
      "Epoch 31\n",
      "----------------------------\n",
      "training loss: 32.770699\n",
      "Validation loss: 36.762196 \n",
      "\n",
      "Epoch 32\n",
      "----------------------------\n",
      "training loss: 31.859827\n",
      "Validation loss: 38.676150 \n",
      "\n",
      "Epoch 33\n",
      "----------------------------\n",
      "training loss: 32.317161\n",
      "Validation loss: 37.472259 \n",
      "\n",
      "Epoch 34\n",
      "----------------------------\n",
      "training loss: 32.684313\n",
      "Validation loss: 38.344685 \n",
      "\n",
      "Epoch 35\n",
      "----------------------------\n",
      "training loss: 31.534580\n",
      "Validation loss: 37.834665 \n",
      "\n",
      "Epoch 36\n",
      "----------------------------\n",
      "training loss: 31.486257\n",
      "Validation loss: 36.736941 \n",
      "\n",
      "Epoch 37\n",
      "----------------------------\n",
      "training loss: 32.059493\n",
      "Validation loss: 36.703642 \n",
      "\n",
      "Epoch 38\n",
      "----------------------------\n",
      "training loss: 32.127826\n",
      "Validation loss: 37.754635 \n",
      "\n",
      "Epoch 39\n",
      "----------------------------\n",
      "training loss: 31.288011\n",
      "Validation loss: 37.156201 \n",
      "\n",
      "Epoch 40\n",
      "----------------------------\n",
      "training loss: 31.066813\n",
      "Validation loss: 37.428740 \n",
      "\n",
      "Epoch 41\n",
      "----------------------------\n",
      "training loss: 30.804414\n",
      "Validation loss: 36.788726 \n",
      "\n",
      "Epoch 42\n",
      "----------------------------\n",
      "training loss: 30.975955\n",
      "Validation loss: 36.501443 \n",
      "\n",
      "Epoch 43\n",
      "----------------------------\n",
      "training loss: 30.635799\n",
      "Validation loss: 36.201230 \n",
      "\n",
      "Epoch 44\n",
      "----------------------------\n",
      "training loss: 31.074895\n",
      "Validation loss: 37.054103 \n",
      "\n",
      "Epoch 45\n",
      "----------------------------\n",
      "training loss: 30.986224\n",
      "Validation loss: 37.263090 \n",
      "\n",
      "Epoch 46\n",
      "----------------------------\n",
      "training loss: 30.860274\n",
      "Validation loss: 37.664730 \n",
      "\n",
      "Epoch 47\n",
      "----------------------------\n",
      "training loss: 30.466474\n",
      "Validation loss: 38.104957 \n",
      "\n",
      "Epoch 48\n",
      "----------------------------\n",
      "training loss: 30.563316\n",
      "Validation loss: 36.752316 \n",
      "\n",
      "Epoch 49\n",
      "----------------------------\n",
      "training loss: 29.563634\n",
      "Validation loss: 38.488929 \n",
      "\n",
      "Epoch 50\n",
      "----------------------------\n",
      "training loss: 30.044165\n",
      "Validation loss: 37.606866 \n",
      "\n",
      "Epoch 51\n",
      "----------------------------\n",
      "training loss: 29.743341\n",
      "Validation loss: 36.571883 \n",
      "\n",
      "Epoch 52\n",
      "----------------------------\n",
      "training loss: 29.816259\n",
      "Validation loss: 37.031436 \n",
      "\n",
      "Epoch 53\n",
      "----------------------------\n",
      "training loss: 29.579885\n",
      "Validation loss: 36.424917 \n",
      "\n",
      "Epoch 54\n",
      "----------------------------\n",
      "training loss: 29.877605\n",
      "Validation loss: 37.575715 \n",
      "\n",
      "Epoch 55\n",
      "----------------------------\n",
      "training loss: 30.154130\n",
      "Validation loss: 40.656949 \n",
      "\n",
      "Epoch 56\n",
      "----------------------------\n",
      "training loss: 29.684297\n",
      "Validation loss: 36.327878 \n",
      "\n",
      "Epoch 57\n",
      "----------------------------\n",
      "training loss: 29.285619\n",
      "Validation loss: 36.292048 \n",
      "\n",
      "Epoch 58\n",
      "----------------------------\n",
      "training loss: 29.311539\n",
      "Validation loss: 34.764641 \n",
      "\n",
      "Epoch 59\n",
      "----------------------------\n",
      "training loss: 29.800428\n",
      "Validation loss: 37.195244 \n",
      "\n",
      "Epoch 60\n",
      "----------------------------\n",
      "training loss: 29.368331\n",
      "Validation loss: 36.172596 \n",
      "\n",
      "Epoch 61\n",
      "----------------------------\n",
      "training loss: 29.032902\n",
      "Validation loss: 35.220577 \n",
      "\n",
      "Epoch 62\n",
      "----------------------------\n",
      "training loss: 29.372244\n",
      "Validation loss: 36.525542 \n",
      "\n",
      "Epoch 63\n",
      "----------------------------\n",
      "training loss: 29.197934\n",
      "Validation loss: 37.609294 \n",
      "\n",
      "Epoch 64\n",
      "----------------------------\n",
      "training loss: 28.786805\n",
      "Validation loss: 36.118397 \n",
      "\n",
      "Epoch 65\n",
      "----------------------------\n",
      "training loss: 28.765667\n",
      "Validation loss: 36.406057 \n",
      "\n",
      "Epoch 66\n",
      "----------------------------\n",
      "training loss: 29.081571\n",
      "Validation loss: 34.830179 \n",
      "\n",
      "Epoch 67\n",
      "----------------------------\n",
      "training loss: 29.149722\n",
      "Validation loss: 36.163946 \n",
      "\n",
      "Epoch 68\n",
      "----------------------------\n",
      "training loss: 28.204909\n",
      "Validation loss: 35.284968 \n",
      "\n",
      "Epoch 69\n",
      "----------------------------\n",
      "training loss: 28.513888\n",
      "Validation loss: 35.680472 \n",
      "\n",
      "Epoch 70\n",
      "----------------------------\n",
      "training loss: 28.560285\n",
      "Validation loss: 35.934070 \n",
      "\n",
      "Epoch 71\n",
      "----------------------------\n",
      "training loss: 28.615933\n",
      "Validation loss: 36.498646 \n",
      "\n",
      "Epoch 72\n",
      "----------------------------\n",
      "training loss: 28.569766\n",
      "Validation loss: 35.681897 \n",
      "\n",
      "Epoch 73\n",
      "----------------------------\n",
      "training loss: 27.862787\n",
      "Validation loss: 34.992320 \n",
      "\n",
      "Epoch 74\n",
      "----------------------------\n",
      "training loss: 27.897627\n",
      "Validation loss: 34.481850 \n",
      "\n",
      "Epoch 75\n",
      "----------------------------\n",
      "training loss: 28.422383\n",
      "Validation loss: 34.772059 \n",
      "\n",
      "Epoch 76\n",
      "----------------------------\n",
      "training loss: 27.757619\n",
      "Validation loss: 36.660809 \n",
      "\n",
      "Epoch 77\n",
      "----------------------------\n",
      "training loss: 28.425846\n",
      "Validation loss: 36.131666 \n",
      "\n",
      "Epoch 78\n",
      "----------------------------\n",
      "training loss: 27.415836\n",
      "Validation loss: 36.428992 \n",
      "\n",
      "Epoch 79\n",
      "----------------------------\n",
      "training loss: 28.119447\n",
      "Validation loss: 35.203121 \n",
      "\n",
      "Epoch 80\n",
      "----------------------------\n",
      "training loss: 28.450989\n",
      "Validation loss: 36.193805 \n",
      "\n",
      "Epoch 81\n",
      "----------------------------\n",
      "training loss: 27.727888\n",
      "Validation loss: 34.941421 \n",
      "\n",
      "Epoch 82\n",
      "----------------------------\n",
      "training loss: 27.935683\n",
      "Validation loss: 35.608540 \n",
      "\n",
      "Epoch 83\n",
      "----------------------------\n",
      "training loss: 27.634582\n",
      "Validation loss: 35.472103 \n",
      "\n",
      "Epoch 84\n",
      "----------------------------\n",
      "training loss: 28.009078\n",
      "Validation loss: 35.169113 \n",
      "\n",
      "Epoch 85\n",
      "----------------------------\n",
      "training loss: 27.298036\n",
      "Validation loss: 35.652967 \n",
      "\n",
      "Epoch 86\n",
      "----------------------------\n",
      "training loss: 27.622556\n",
      "Validation loss: 35.558467 \n",
      "\n",
      "Epoch 87\n",
      "----------------------------\n",
      "training loss: 27.277225\n",
      "Validation loss: 36.170966 \n",
      "\n",
      "Epoch 88\n",
      "----------------------------\n",
      "training loss: 27.402765\n",
      "Validation loss: 35.202939 \n",
      "\n",
      "Epoch 89\n",
      "----------------------------\n",
      "training loss: 28.079649\n",
      "Validation loss: 35.942081 \n",
      "\n",
      "Epoch 90\n",
      "----------------------------\n",
      "training loss: 26.746662\n",
      "Validation loss: 35.201397 \n",
      "\n",
      "Epoch 91\n",
      "----------------------------\n",
      "training loss: 27.006465\n",
      "Validation loss: 35.429492 \n",
      "\n",
      "Epoch 92\n",
      "----------------------------\n",
      "training loss: 26.893000\n",
      "Validation loss: 36.543734 \n",
      "\n",
      "Epoch 93\n",
      "----------------------------\n",
      "training loss: 26.866070\n",
      "Validation loss: 35.401916 \n",
      "\n",
      "Epoch 94\n",
      "----------------------------\n",
      "training loss: 26.863406\n",
      "Validation loss: 35.471756 \n",
      "\n",
      "Epoch 95\n",
      "----------------------------\n",
      "training loss: 27.109934\n",
      "Validation loss: 35.267778 \n",
      "\n",
      "Epoch 96\n",
      "----------------------------\n",
      "training loss: 26.838739\n",
      "Validation loss: 36.309244 \n",
      "\n",
      "Epoch 97\n",
      "----------------------------\n",
      "training loss: 27.295731\n",
      "Validation loss: 35.659752 \n",
      "\n",
      "Epoch 98\n",
      "----------------------------\n",
      "training loss: 26.742542\n",
      "Validation loss: 35.907784 \n",
      "\n",
      "Epoch 99\n",
      "----------------------------\n",
      "training loss: 26.373328\n",
      "Validation loss: 34.844262 \n",
      "\n",
      "Epoch 100\n",
      "----------------------------\n",
      "training loss: 26.516821\n",
      "Validation loss: 35.959401 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-fold Cross Validation model evaluation\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_data)):\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = SubsetRandomSampler(train_ids)\n",
    "    val_subsampler = SubsetRandomSampler(val_ids)\n",
    "    \n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batchsize, sampler=train_subsampler)\n",
    "    val_dataloader = DataLoader(train_data, batch_size=batchsize, sampler=val_subsampler)\n",
    "\n",
    "    if MODEL_FLAG == 'cnn':\n",
    "        model = OneDCNN()\n",
    "    elif MODEL_FLAG == 'transformer':\n",
    "        model = OneDTransformer(\n",
    "            n_classes=256, \n",
    "            n_label=1,\n",
    "            n_length=95,\n",
    "            n_channel=1,\n",
    "            vocab_size=100, \n",
    "            num_embed=32,\n",
    "            embed_size=512,\n",
    "            nhead=8, \n",
    "            dim_feedforward=1024, \n",
    "            dropout=0.25,\n",
    "            activation='relu',\n",
    "            verbose=False\n",
    "        )\n",
    "    elif MODEL_FLAG == 'mlp':\n",
    "        model = OurMLP(num_classes=1, in_channels=95)\n",
    "    elif MODEL_FLAG == 'attention':\n",
    "        model = OneAttention()\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-8)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    losses_train: list = []\n",
    "    losses_val: list = []\n",
    "    tr_size = len(train_dataloader)\n",
    "    val_size = len(val_dataloader)\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n----------------------------\")\n",
    "        tr_loss = 0\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dataloader):\n",
    "            if MODEL_FLAG == 'transformer':\n",
    "                # X, y = X.long().to(device), y.to(device)\n",
    "                X, y = X.to(device), y.to(device)\n",
    "            else:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute the prediction error\n",
    "            pred = model(X)\n",
    "            # print(y)\n",
    "            # print(pred)\n",
    "            # loss = criterion(pred, y) # MSE\n",
    "            loss = torch.sqrt(criterion(pred, y)) # RMSE\n",
    "\n",
    "            # break\n",
    "            \n",
    "            # Reset the gradient to 0 after each epoch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Compute the backpropagation (back propagate the loss and add one step to the optimizer)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss = loss.item()\n",
    "            tr_loss += loss\n",
    "\n",
    "        print(f\"training loss: {tr_loss/tr_size:>7f}\")\n",
    "        losses_train.append(tr_loss/tr_size)\n",
    "    \n",
    "        # validation\n",
    "        # num_batches = len(val_dataloader)\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                pred = model(X)\n",
    "                loss = torch.sqrt(criterion(pred, y)).item() # RMSE\n",
    "                # loss = criterion(pred, y).item() # MSE\n",
    "                val_loss += loss\n",
    "                # break\n",
    "        losses_val.append(val_loss/val_size)\n",
    "        print(f\"Validation loss: {val_loss/val_size:>8f} \\n\")\n",
    "\n",
    "    # save loss locally\n",
    "    with open(f'{log_dir}/{fold}_train_loss.obj', 'wb') as fp:\n",
    "        pickle.dump(losses_train, fp)\n",
    "\n",
    "    with open(f'{log_dir}/{fold}_val_loss.obj', 'wb') as fp:\n",
    "        pickle.dump(losses_val, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following code is not needed when k-fold validation is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# losses_train: list = []\n",
    "# losses_val: list = []\n",
    "# tr_size = len(train_dataloader.dataset)\n",
    "# val_size = len(val_dataloader.dataset)\n",
    "\n",
    "# for epoch in range(0, num_epochs):\n",
    "#     print(f\"Epoch {epoch + 1}\\n----------------------------\")\n",
    "#     tr_loss = 0\n",
    "#     model.train()\n",
    "#     for batch, (X, y) in enumerate(train_dataloader):\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "\n",
    "#         # Compute the prediction error\n",
    "#         pred = model(X)\n",
    "#         loss = torch.sqrt(criterion(pred, y)) # RMSE\n",
    "#         # loss = criterion(pred, y) # MSE\n",
    "        \n",
    "#         # Reset the gradient to 0 after each epoch\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Compute the backpropagation (back propagate the loss and add one step to the optimizer)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loss = loss.item()\n",
    "#         tr_loss += loss\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             print(f\"training loss: {tr_loss/tr_size:>7f}\")\n",
    "        \n",
    "#     losses_train.append(tr_loss/tr_size)\n",
    "\n",
    "#     # validation\n",
    "#     num_batches = len(val_dataloader)\n",
    "#     val_loss = 0\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for X, y in val_dataloader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "#             pred = model(X)\n",
    "#             loss = torch.sqrt(criterion(pred, y)).item() # RMSE\n",
    "#             # loss = criterion(pred, y).item() # MSE\n",
    "#             val_loss += loss\n",
    "#     losses_val.append(val_loss/val_size)\n",
    "#     print(f\"Validation loss: {val_loss/val_size:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the last k-fold training loss and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_x = np.arange(0, len(losses_train), 1, dtype=int)\n",
    "axis_y = losses_train\n",
    "\n",
    "axis_val_x = np.arange(0, len(losses_val), 1, dtype=int)\n",
    "axis_val_y = losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the chart\n",
    "plt.plot(axis_x, axis_y, label=\"train\")\n",
    "plt.plot(axis_val_x, axis_val_y, label=\"validation\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()  # display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following codes are not needed if k-fold validation is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_array = X_test.iloc[:, :-1].to_numpy()\n",
    "test_y_array = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_array = scaler.transform(test_X_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = RadiomicDataset(test_X_array, test_y_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 36.419410 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_size = len(test_dataloader)\n",
    "num_batches = len(test_dataloader)\n",
    "test_loss = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = torch.sqrt(criterion(pred, y)).item()\n",
    "        # loss = criterion(pred, y).item()\n",
    "        test_loss += loss\n",
    "print(f\"Test loss: {test_loss/test_size:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for X, y in test_dataloader:\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "#         pred = model(X)\n",
    "#         print(torch.squeeze(pred))\n",
    "#         print(torch.squeeze(y))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Case Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ind = random.sample(range(0, len(test_X_array)), 20)\n",
    "# ind = random.randint(0, len(test_X_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image:  Patient00713_Plane3_4_of_4.png\n",
      "Y:      208.7171928\n",
      "Pred_Y: 193.452880859375\n",
      "--------------------------------------------------\n",
      "Image:  358_HC.png\n",
      "Y:      138.7664598\n",
      "Pred_Y: 93.44202423095703\n",
      "--------------------------------------------------\n",
      "Image:  372_HC.png\n",
      "Y:      140.0331919\n",
      "Pred_Y: 116.88724517822266\n",
      "--------------------------------------------------\n",
      "Image:  133_2HC.png\n",
      "Y:      98.97632485\n",
      "Pred_Y: 135.3134002685547\n",
      "--------------------------------------------------\n",
      "Image:  Patient01048_Plane3_1_of_1.png\n",
      "Y:      152.370088\n",
      "Pred_Y: 185.22068786621094\n",
      "--------------------------------------------------\n",
      "Image:  529_HC.png\n",
      "Y:      140.7641374\n",
      "Pred_Y: 173.9601287841797\n",
      "--------------------------------------------------\n",
      "Image:  Patient01209_Plane3_1_of_7.png\n",
      "Y:      162.7116183\n",
      "Pred_Y: 145.47232055664062\n",
      "--------------------------------------------------\n",
      "Image:  Patient00896_Plane3_1_of_3.png\n",
      "Y:      142.5998953\n",
      "Pred_Y: 140.92779541015625\n",
      "--------------------------------------------------\n",
      "Image:  030_HC.png\n",
      "Y:      78.44650892\n",
      "Pred_Y: 121.37918853759766\n",
      "--------------------------------------------------\n",
      "Image:  260_2HC.png\n",
      "Y:      132.2462289\n",
      "Pred_Y: 149.88279724121094\n",
      "--------------------------------------------------\n",
      "Image:  Patient01293_Plane3_4_of_6.png\n",
      "Y:      105.6331707\n",
      "Pred_Y: 136.4757080078125\n",
      "--------------------------------------------------\n",
      "Image:  712_HC.png\n",
      "Y:      195.4288364\n",
      "Pred_Y: 178.60801696777344\n",
      "--------------------------------------------------\n",
      "Image:  397_HC.png\n",
      "Y:      137.3259411\n",
      "Pred_Y: 136.99496459960938\n",
      "--------------------------------------------------\n",
      "Image:  671_3HC.png\n",
      "Y:      175.8537876\n",
      "Pred_Y: 103.74148559570312\n",
      "--------------------------------------------------\n",
      "Image:  437_2HC.png\n",
      "Y:      141.3337667\n",
      "Pred_Y: 141.24862670898438\n",
      "--------------------------------------------------\n",
      "Image:  Patient00780_Plane3_1_of_1.png\n",
      "Y:      228.3576018\n",
      "Pred_Y: 243.44375610351562\n",
      "--------------------------------------------------\n",
      "Image:  Patient01128_Plane3_1_of_3.png\n",
      "Y:      260.4725245\n",
      "Pred_Y: 144.46853637695312\n",
      "--------------------------------------------------\n",
      "Image:  Patient00305_Plane3_3_of_5.png\n",
      "Y:      163.7979846\n",
      "Pred_Y: 154.4998321533203\n",
      "--------------------------------------------------\n",
      "Image:  Patient00936_Plane3_1_of_1.png\n",
      "Y:      138.598351\n",
      "Pred_Y: 152.7592315673828\n",
      "--------------------------------------------------\n",
      "Image:  Patient00882_Plane3_1_of_3.png\n",
      "Y:      206.6898203\n",
      "Pred_Y: 206.4907684326172\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in ind:\n",
    "    single_test = torch.tensor(test_X_array[i]).unsqueeze(0).float()\n",
    "    single_test = single_test.unsqueeze(1)\n",
    "    single_y_pred = model(single_test.to(device))\n",
    "    # single_y_pred.item()\n",
    "    single_y = test_y_array[i]\n",
    "\n",
    "    print(f'Image:  {X_test.iloc[i,-1]}')\n",
    "    print(f'Y:      {single_y}')\n",
    "    print(f'Pred_Y: {single_y_pred.item()}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "mamba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
